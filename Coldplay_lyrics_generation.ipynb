{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Coldplay lyrics generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruiwen829/Coldplay-lyrics-Generation/blob/main/Coldplay_lyrics_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Lyrics generation with an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcD2nPQvPOFM"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "source": [
        "import tensorflow as tf\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### Upload the Coldplay lyrics dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD_55cOxLkAb"
      },
      "source": [
        "text_all = \"\"\n",
        "for root, dirs, files in os.walk(os.getcwd()):\n",
        "    for file in files:\n",
        "        if file.endswith('.txt'):\n",
        "            with open(os.path.join(root, file), 'r') as f:\n",
        "                text = f.read()\n",
        "                text_all+=text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HMUkBTYG-KN"
      },
      "source": [
        "delete_letters = ['è', 'ê', 'í', 'ó']\n",
        "text_all = ' '.join([text for text in text_all.split() if all(d not in text for d in delete_letters)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data\n",
        "\n",
        "First, look in the text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aavnuByVymwK",
        "outputId": "cac82454-b750-43b5-d586-02b74112df39"
      },
      "source": [
        "# length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text_all)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 27992 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Duhg9NrUymwO",
        "outputId": "a46bbf31-8623-4e60-8be9-fd9cbfcfe486"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text_all[:250])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Come up to meet you, Tell you I'm sorry, You don't know how lovely you are. I had to find you, Tell you I need you, Tell you I set you apart. Tell me your secrets, And ask me your questions, Oh let's go back to the start. Runnin' in circles, Comin' u\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlCgQBRVymwR",
        "outputId": "c944b7c3-f41b-4c7f-9732-4fb6a3c8c045"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text_all))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IalZLbvOzf-F"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text_all])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "Now we have an integer representation for each character. Notice that the character was indexed from 0 to `len(unique)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYyNlCNXymwY",
        "outputId": "9f8a7246-a96b-49b5-fa3b-bab5d65b7b09"
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  ' ' :   0,\n",
            "  '!' :   1,\n",
            "  '\"' :   2,\n",
            "  \"'\" :   3,\n",
            "  '(' :   4,\n",
            "  ')' :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '0' :   9,\n",
            "  '1' :  10,\n",
            "  '2' :  11,\n",
            "  '3' :  12,\n",
            "  '4' :  13,\n",
            "  ':' :  14,\n",
            "  ';' :  15,\n",
            "  '?' :  16,\n",
            "  'A' :  17,\n",
            "  'B' :  18,\n",
            "  'C' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1VKcQHcymwb",
        "outputId": "a2c529b0-3a44-4f4f-b34c-fd4c96ac9635"
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print('{} ---- characters mapped to int ---- > {}'.format(repr(text_all[:13]), text_as_int[:13]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Come up to me' ---- characters mapped to int ---- > [19 57 55 47  0 63 58  0 62 57  0 55 47]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### The prediction task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task you're training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Create training examples and targets\n",
        "\n",
        "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UHJDA39zf-O",
        "outputId": "84328681-f497-48f1-bfa2-6a07146c8624"
      },
      "source": [
        "# The maximum length sentence you want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text_all)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C\n",
            "o\n",
            "m\n",
            "e\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "The `batch` method lets us easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4hkDU3i7ozi",
        "outputId": "27946300-529e-4b02-dfab-4f7592421bbb"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Come up to meet you, Tell you I'm sorry, You don't know how lovely you are. I had to find you, Tell y\"\n",
            "\"ou I need you, Tell you I set you apart. Tell me your secrets, And ask me your questions, Oh let's go\"\n",
            "\" back to the start. Runnin' in circles, Comin' up tails, Heads on a science apart. Nobody said it was\"\n",
            "\" easy, It's such a shame for us to part. Nobody said it was easy, No one ever said it would be this h\"\n",
            "\"ard. Oh take me back to the start. I was just guessin', At numbers and figures, Pullin' the puzzles a\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "For each sequence, duplicate and shift it to form the input and target text by using the `map` method to apply a simple function to each batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiCopyGZymwi"
      },
      "source": [
        "Print the first example input and target values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNbw-iR0ymwj",
        "outputId": "eba6fdc5-ffaa-42a7-9821-7c7ec2d97125"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  \"Come up to meet you, Tell you I'm sorry, You don't know how lovely you are. I had to find you, Tell \"\n",
            "Target data: \"ome up to meet you, Tell you I'm sorry, You don't know how lovely you are. I had to find you, Tell y\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_33OHL3b84i0"
      },
      "source": [
        "Each index of these vectors is processed as a one time step. For the input at time step 0, the model receives the index for \"F\" and tries to predict the index for \"i\" as the next character. At the next timestep, it does the same thing but the `RNN` considers the previous step context in addition to the current input character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eBu9WZG84i0",
        "outputId": "c332e22d-8edb-49e5-9f79-d347e6135ebc"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 19 ('C')\n",
            "  expected output: 57 ('o')\n",
            "Step    1\n",
            "  input: 57 ('o')\n",
            "  expected output: 55 ('m')\n",
            "Step    2\n",
            "  input: 55 ('m')\n",
            "  expected output: 47 ('e')\n",
            "Step    3\n",
            "  input: 47 ('e')\n",
            "  expected output: 0 (' ')\n",
            "Step    4\n",
            "  input: 0 (' ')\n",
            "  expected output: 63 ('u')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "You used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2pGotuNzf-S",
        "outputId": "8acda2af-f09f-4053-c702-37e1014a97e7"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "Use `tf.keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtCrdfzEI2N0"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwsrpOik5zhv"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
        "\n",
        "![A drawing of the data passing through the model](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/text_generation_training.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKbfm04amhXk"
      },
      "source": [
        "Please note that Keras sequential model is used here since all the layers in the model only have single input and produce single output. In case you want to retrieve and reuse the states from stateful RNN layer, you might want to build your model with Keras functional API or model subclassing. Please check [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## Try the model\n",
        "\n",
        "Run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-_70kKAPrPU",
        "outputId": "047bcab7-e3f7-4da4-cf71-4a0690bebef8"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 70) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "In the above example the sequence length of the input is `100` but the model can be run on inputs of any length:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPGmAAXmVLGC",
        "outputId": "da03911d-68f4-48ac-9ea5-22a17762e376"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           17920     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 70)            71750     \n",
            "=================================================================\n",
            "Total params: 4,027,974\n",
            "Trainable params: 4,027,974\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
        "\n",
        "Try it for the first example in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V4MfFg0RQJg"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "This gives us, at each timestep, a prediction of the next character index:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqFMUQc_UFgM",
        "outputId": "a6ba70a1-2dd5-442b-d802-abf2cf395809"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([58, 66,  1, 45, 32, 22, 44,  7,  9, 51,  0, 24, 37, 12, 46,  0, 11,\n",
              "       30, 42, 17, 58, 12, 67, 51,  8, 26, 55, 61,  0, 48, 58,  0, 64, 68,\n",
              "       57, 17,  0, 23, 11, 61,  1,  1, 49,  1, 62, 43, 12, 68, 25, 61, 56,\n",
              "       15, 61, 25, 55, 38, 47,  6, 15, 43,  5, 60, 15, 15, 28, 36, 21, 36,\n",
              "       24, 54, 52, 47, 29, 41, 50, 56, 29,  6, 41, 49, 52, 15, 15, 64, 40,\n",
              "       18, 54, 20, 49, 37, 25, 11, 50,  3, 15, 45, 13, 51, 26,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "Decode these to see the text predicted by this untrained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWcFwPwLSo05",
        "outputId": "2d282c1f-4951-45c4-f674-037d52bedc2d"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " ' bubble. And I never meant to cause you trouble, Oh I never meant to do you wrong, And I, well if I '\n",
            "\n",
            "Next Char Predictions: \n",
            " \"px!cPFb-0i HU3d 2N]Ap3yi.Jms fp vzoA G2s!!g!ta3zIsn;sImVe,;a)r;;LTETHljeM[hnM,[gj;;vYBlDgUI2h';c4iJ \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Attach an optimizer, and a loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because your model returns logits, you need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HrXTACTdzY-",
        "outputId": "ea5ca0ff-a114-4a97-a024-73858ef7ecf2"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 70)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.249137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "source": [
        "EPOCHS = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "UK-hmKjYVoll",
        "outputId": "4c4419dd-cdbe-42c1-d52a-b904c02c183c"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-25e345c13e8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:187 __call__\n        self.build(y_pred)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:140 build\n        self._losses = nest.map_structure(self._get_loss_object, self._losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:635 map_structure\n        structure[0], [func(*x) for x in entries],\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:635 <listcomp>\n        structure[0], [func(*x) for x in entries],\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:263 _get_loss_object\n        loss = losses_mod.get(loss)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1902 get\n        'Could not interpret loss function identifier: {}'.format(identifier))\n\n    ValueError: Could not interpret loss function identifier: 2.873863935470581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIPcXllKjkdr"
      },
      "source": [
        "### Restore the latest checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyeYRiuVjodY"
      },
      "source": [
        "To keep this prediction step simple, use a batch size of 1.\n",
        "\n",
        "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
        "\n",
        "To run the model with a different `batch_size`, you need to rebuild the model and restore the weights from the checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "zk2WJ2-XjkGz",
        "outputId": "47968d93-24da-48a3-9311-4b753416c426"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LycQ-ot_jjyu",
        "outputId": "e53cb30c-852f-413e-8c38-1f9474386f94"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71xa6jnYVrAN",
        "outputId": "8ea2af5c-f194-40e7-fb30-b7690e1cba06"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            17920     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 70)             71750     \n",
            "=================================================================\n",
            "Total params: 4,027,974\n",
            "Trainable params: 4,027,974\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "### The prediction loop\n",
        "\n",
        "The following code block generates the text:\n",
        "\n",
        "* Begin by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
        "\n",
        "* Get the prediction distribution of the next character using the start string and the RNN state.\n",
        "\n",
        "* Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
        "\n",
        "* The RNN state returned by the model is fed back into the model so that it now has more context, instead of only one character. After predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters.\n",
        "\n",
        "\n",
        "![To generate text the model's output is fed back to the input](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/text_generation_sampling.png?raw=1)\n",
        "\n",
        "Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvuwZBX5Ogfd"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 1000\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 1.0\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktovv0RFhrkn",
        "outputId": "a91db26d-37ed-40eb-983f-98c51b36c03c"
      },
      "source": [
        "print(generate_text(model, start_string=u\"sky\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sky,, gaoit d eMte a tritdge stgiyllnimd I to we l iI on  nt wamke tsn Taudmi iO tserideanUSAa]'; wo 't dr m udtn efey ioasispsto o gmheo et wku 'hyai'nA whyore onVpbt oyiml, ytbejhrwrr hu rpr yindds aoMNuiy nhmreronerrt ignues gon ol hag n' raesahoe uivt etae d, v k wn ao [ot es inmh e ia I yovtriac, w sltn ecirenp I Mwh l Coi nr e li s deos Iw,n aOzCOat pu s y teana t  t so, unn f tt,nnnovecrkuun Sm oyin ians t e t is oesehrs tgst [olL ld ste  at es ibfptenodVado, r. cis tsm yly t shBe soemoe nw,s temu teis - d?tnprohog y'Gcsonr y od's I t p Oh gat aAwd inne b la ikyfayu a r snl'r tnaie AlyWs ys oonee unuer rn saodntna[lco cng, Slne cn anna.md utr-eueVed is uh wo hhaye td lonolnnghJcDuowSvhgd 't n .r urti niufahtcnrs my i Ae h it etemslt go tlalleek Ion  twoe ooevdnf  tylc[\"fghe mpegin Inbl ee scnnvuu rrr r an-wsn  eon C m os'u svl  o e s ss e i tsfacit t ua ty lhatmre hrte elnrs nmdIl Yhvye e d yndVano tue ug hn wdmt h tI iacd se turn iogse'le rtI rricl ieolscrhtec iaft oiat os 'g e ut \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "The easiest thing you can do to improve the results is to train it for longer.\n",
        "\n",
        "We can also predict with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions.\n",
        "\n",
        "But I decided to switch to a Word-based Model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm-Mly_C3qWW"
      },
      "source": [
        "## Word-based model with LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAD4VMGiJBcX"
      },
      "source": [
        "### Train an ad hoc word embedding.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wha3WoaU7qeu",
        "outputId": "fbf4f6df-07d8-4c96-fcc3-d663d6d66195"
      },
      "source": [
        "import nltk \n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "850am3tFC5CB"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Activation, LSTM, Dropout, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOQzOJGj_xR7"
      },
      "source": [
        "### Word-Level Model with LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz7uXd-T5HUx"
      },
      "source": [
        "file = open('/content/lyrics_for_generation.txt')\n",
        "lyrics = file.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7UO7uztH_oh"
      },
      "source": [
        "lyrics = lyrics[:len(lyrics)-215] # get rid of spanish characters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5ns4gJSKn5q"
      },
      "source": [
        "from string import punctuation\n",
        "import re\n",
        " \n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# replace '--' with a space ' '\n",
        "\tdoc = doc.replace('--', ' ')\n",
        " \n",
        "\t# remove punctuation from each token but keep '\\n' and '''\n",
        "\tmy_punctuation = punctuation.replace(\"\\\\\", \"\")\n",
        "\tmy_punctuation = punctuation.replace(\"'\", \"\")\n",
        "  \n",
        "\tdoc = doc.translate(str.maketrans(\"\", \"\", my_punctuation))\n",
        " \n",
        " \t# split into tokens but keep '\\n' as a token\n",
        "\ttokens = re.findall(r'\\S+|\\n',doc)\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [x for x in tokens if not x.isdigit()]\n",
        "\t# make lower case\n",
        "\ttokens = [word.lower() for word in tokens]\n",
        "\treturn tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBjTZnXh5RvY"
      },
      "source": [
        "tokens = clean_doc(lyrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp3F8LDa5YUz",
        "outputId": "055fe595-0d6a-44d1-f701-b93fb8f35a74"
      },
      "source": [
        "print(tokens[:200])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'used', 'to', 'rule', 'the', 'world', '\\n', 'seas', 'would', 'rise', 'when', 'i', 'gave', 'the', 'word', '\\n', 'now', 'in', 'the', 'morning', 'i', 'sleep', 'alone', '\\n', 'sweep', 'the', 'streets', 'i', 'used', 'to', 'own', '\\n', '\\n', 'i', 'used', 'to', 'roll', 'the', 'dice', '\\n', 'feel', 'the', 'fear', 'in', 'my', \"enemy's\", 'eyes', '\\n', 'listen', 'as', 'the', 'crowd', 'would', 'sing', '\\n', 'now', 'the', 'old', 'king', 'is', 'dead', 'long', 'live', 'the', 'king', '\\n', '\\n', 'one', 'minute', 'i', 'held', 'the', 'key', '\\n', 'next', 'the', 'walls', 'were', 'closed', 'on', 'me', '\\n', 'and', 'i', 'discovered', 'that', 'my', 'castles', 'stand', '\\n', 'upon', 'pillars', 'of', 'salt', 'and', 'pillars', 'of', 'sand', '\\n', '\\n', 'i', 'hear', 'jerusalem', 'bells', 'are', 'ringing', '\\n', 'roman', 'cavalry', 'choirs', 'are', 'singing', '\\n', 'be', 'my', 'mirror', 'my', 'sword', 'and', 'shield', '\\n', 'my', 'missionaries', 'in', 'a', 'foreign', 'field', '\\n', '\\n', 'for', 'some', 'reason', 'i', \"can't\", 'explain', '\\n', 'once', 'you', 'go', 'there', 'was', 'never', '\\n', 'never', 'an', 'honest', 'word', '\\n', 'and', 'that', 'was', 'when', 'i', 'ruled', 'the', 'world', '\\n', '\\n', 'it', 'was', 'the', 'wicked', 'and', 'wild', 'wind', '\\n', 'blew', 'down', 'the', 'doors', 'to', 'let', 'me', 'in', '\\n', 'shattered', 'windows', 'and', 'the', 'sound', 'of', 'drums', '\\n', 'people', \"couldn't\", 'believe', 'what', \"i'd\", 'become', '\\n', '\\n', 'revolutionaries', 'wait', '\\n', 'for', 'my', 'head', 'on', 'a', 'silver']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QFCf3rt5eqo",
        "outputId": "81b6e8bd-8120-4c61-a551-b874110bce12"
      },
      "source": [
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Tokens: 10006\n",
            "Unique Tokens: 1231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w8FTKEm5jON"
      },
      "source": [
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "def save_doc(lines, filename):\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkRbU6Fl_tDJ",
        "outputId": "ef7e032c-7cab-42f8-f90e-37b1fd1bca28"
      },
      "source": [
        "# organize into sequences of tokens\n",
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "\t# select sequence of tokens\n",
        "\tseq = tokens[i-length:i]\n",
        "\t# convert into a line\n",
        "\tline = ' '.join(seq)\n",
        "\t# store\n",
        "\tsequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))\n",
        " \n",
        "# save sequences to file\n",
        "out_filename = 'lyrics_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 9955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHIz-mI5EbO4"
      },
      "source": [
        "# integer encode sequences of words\n",
        "# import tensorflow as tf\n",
        "# tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "# tokenizer.fit_on_texts(sequences)\n",
        "# sequences = tokenizer.texts_to_sequences(sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfXbedI5T0j5"
      },
      "source": [
        "word2num = {}\n",
        "for idx, word in enumerate(set(tokens)):\n",
        "  word2num[word] = idx+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eytihradVmg8"
      },
      "source": [
        "seqs = []\n",
        "for sequence in sequences:\n",
        "  seqs.append([word2num[w] for w in re.findall(r'\\S+|\\n',sequence)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TeZpZjxIrYB"
      },
      "source": [
        "Words are assigned values from 1 to the total number of words (e.g. 7,409). The Embedding layer needs to allocate a vector representation for each word in this vocabulary from index 1 to the largest index and because indexing of arrays is zero-offset, the index of the word at the end of the vocabulary will be 7,409; that means the array must be 7,409 + 1 in length.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYJQHrOZE1NT"
      },
      "source": [
        "# vocabulary size\n",
        "vocab_size = len(set(tokens)) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSuOVKzIt6W4"
      },
      "source": [
        "Keras provides the ``to_categorical()`` that can be used to ***one hot encode*** the output words for each input-output sequence pair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUg6WUooIwwo"
      },
      "source": [
        "# separate into input and output\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "sequences = np.array(seqs)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sww8sZ2Ku4w",
        "outputId": "97590488-9e69-425d-fea7-611767abd5c8"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1062,  992,  336, ...,  501,   74,  545],\n",
              "       [ 992,  336, 1006, ...,   74,  545,   64],\n",
              "       [ 336, 1006,   64, ...,  545,   64,  667],\n",
              "       ...,\n",
              "       [1191,  515,  525, ...,   89,  873,   64],\n",
              "       [ 515,  525,  501, ...,  873,   64,  451],\n",
              "       [ 525,  501, 1202, ...,   64,  451, 1055]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekI-LIvQwkeY",
        "outputId": "8923f38e-204b-404e-ac12-680f52034819"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(100))\n",
        "#model.add(Dropout(0.25))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 50, 50)            61600     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 50, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1232)              124432    \n",
            "=================================================================\n",
            "Total params: 336,932\n",
            "Trainable params: 336,932\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EE_hMlo2UtI",
        "outputId": "a214ee0b-65b3-477c-d08b-87988c82b581"
      },
      "source": [
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=32, epochs=100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "312/312 [==============================] - 28s 90ms/step - loss: 5.5684 - accuracy: 0.1715\n",
            "Epoch 2/100\n",
            "312/312 [==============================] - 30s 96ms/step - loss: 5.1846 - accuracy: 0.1726\n",
            "Epoch 3/100\n",
            "312/312 [==============================] - 30s 95ms/step - loss: 5.0663 - accuracy: 0.1729\n",
            "Epoch 4/100\n",
            "312/312 [==============================] - 30s 95ms/step - loss: 4.8735 - accuracy: 0.1774\n",
            "Epoch 5/100\n",
            "312/312 [==============================] - 30s 96ms/step - loss: 4.6871 - accuracy: 0.1856\n",
            "Epoch 6/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 4.5315 - accuracy: 0.1937\n",
            "Epoch 7/100\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 4.3899 - accuracy: 0.2054\n",
            "Epoch 8/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 4.2658 - accuracy: 0.2096\n",
            "Epoch 9/100\n",
            "312/312 [==============================] - 30s 95ms/step - loss: 4.1435 - accuracy: 0.2183\n",
            "Epoch 10/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 4.0246 - accuracy: 0.2330\n",
            "Epoch 11/100\n",
            "312/312 [==============================] - 32s 102ms/step - loss: 3.9051 - accuracy: 0.2415\n",
            "Epoch 12/100\n",
            "312/312 [==============================] - 32s 101ms/step - loss: 3.7826 - accuracy: 0.2514\n",
            "Epoch 13/100\n",
            "312/312 [==============================] - 31s 100ms/step - loss: 3.6594 - accuracy: 0.2620\n",
            "Epoch 14/100\n",
            "312/312 [==============================] - 31s 99ms/step - loss: 3.5431 - accuracy: 0.2790\n",
            "Epoch 15/100\n",
            "312/312 [==============================] - 31s 100ms/step - loss: 3.4299 - accuracy: 0.2878\n",
            "Epoch 16/100\n",
            "312/312 [==============================] - 31s 101ms/step - loss: 3.3227 - accuracy: 0.3025\n",
            "Epoch 17/100\n",
            "312/312 [==============================] - 31s 99ms/step - loss: 3.2074 - accuracy: 0.3146\n",
            "Epoch 18/100\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 3.1160 - accuracy: 0.3283\n",
            "Epoch 19/100\n",
            "312/312 [==============================] - 31s 101ms/step - loss: 3.0274 - accuracy: 0.3365\n",
            "Epoch 20/100\n",
            "312/312 [==============================] - 31s 101ms/step - loss: 2.9263 - accuracy: 0.3456\n",
            "Epoch 21/100\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 2.8488 - accuracy: 0.3576\n",
            "Epoch 22/100\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 2.7733 - accuracy: 0.3716\n",
            "Epoch 23/100\n",
            "312/312 [==============================] - 32s 102ms/step - loss: 2.6917 - accuracy: 0.3788\n",
            "Epoch 24/100\n",
            "312/312 [==============================] - 31s 100ms/step - loss: 2.6203 - accuracy: 0.3888\n",
            "Epoch 25/100\n",
            "312/312 [==============================] - 31s 99ms/step - loss: 2.5452 - accuracy: 0.4075\n",
            "Epoch 26/100\n",
            "312/312 [==============================] - 31s 100ms/step - loss: 2.4875 - accuracy: 0.4161\n",
            "Epoch 27/100\n",
            "312/312 [==============================] - 31s 101ms/step - loss: 2.4195 - accuracy: 0.4302\n",
            "Epoch 28/100\n",
            "312/312 [==============================] - 32s 104ms/step - loss: 2.3700 - accuracy: 0.4338\n",
            "Epoch 29/100\n",
            "312/312 [==============================] - 32s 102ms/step - loss: 2.3009 - accuracy: 0.4488\n",
            "Epoch 30/100\n",
            "312/312 [==============================] - 32s 102ms/step - loss: 2.2570 - accuracy: 0.4535\n",
            "Epoch 31/100\n",
            "312/312 [==============================] - 33s 105ms/step - loss: 2.1939 - accuracy: 0.4630\n",
            "Epoch 32/100\n",
            "312/312 [==============================] - 32s 103ms/step - loss: 2.1375 - accuracy: 0.4790\n",
            "Epoch 33/100\n",
            "312/312 [==============================] - 31s 99ms/step - loss: 2.0806 - accuracy: 0.4875\n",
            "Epoch 34/100\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 2.0449 - accuracy: 0.4992\n",
            "Epoch 35/100\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 1.9993 - accuracy: 0.5029\n",
            "Epoch 36/100\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 1.9514 - accuracy: 0.5127\n",
            "Epoch 37/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 1.8962 - accuracy: 0.5242\n",
            "Epoch 38/100\n",
            "312/312 [==============================] - 30s 96ms/step - loss: 1.8574 - accuracy: 0.5355\n",
            "Epoch 39/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 1.8095 - accuracy: 0.5465\n",
            "Epoch 40/100\n",
            "312/312 [==============================] - 31s 99ms/step - loss: 1.7782 - accuracy: 0.5478\n",
            "Epoch 41/100\n",
            "312/312 [==============================] - 31s 99ms/step - loss: 1.7337 - accuracy: 0.5602\n",
            "Epoch 42/100\n",
            "312/312 [==============================] - 32s 102ms/step - loss: 1.6933 - accuracy: 0.5700\n",
            "Epoch 43/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 1.6614 - accuracy: 0.5731\n",
            "Epoch 44/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 1.6260 - accuracy: 0.5836\n",
            "Epoch 45/100\n",
            "312/312 [==============================] - 30s 96ms/step - loss: 1.5849 - accuracy: 0.5912\n",
            "Epoch 46/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 1.5388 - accuracy: 0.5997\n",
            "Epoch 47/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 1.5205 - accuracy: 0.6056\n",
            "Epoch 48/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 1.4862 - accuracy: 0.6067\n",
            "Epoch 49/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 1.4539 - accuracy: 0.6197\n",
            "Epoch 50/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 1.4244 - accuracy: 0.6244\n",
            "Epoch 51/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 1.4001 - accuracy: 0.6290\n",
            "Epoch 52/100\n",
            "312/312 [==============================] - 31s 99ms/step - loss: 1.3654 - accuracy: 0.6413\n",
            "Epoch 53/100\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 1.3336 - accuracy: 0.6468\n",
            "Epoch 54/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 1.3101 - accuracy: 0.6551\n",
            "Epoch 55/100\n",
            "312/312 [==============================] - 30s 96ms/step - loss: 1.2771 - accuracy: 0.6554\n",
            "Epoch 56/100\n",
            "312/312 [==============================] - 31s 99ms/step - loss: 1.2737 - accuracy: 0.6525\n",
            "Epoch 57/100\n",
            "312/312 [==============================] - 30s 96ms/step - loss: 1.2291 - accuracy: 0.6658\n",
            "Epoch 58/100\n",
            "312/312 [==============================] - 30s 95ms/step - loss: 1.2135 - accuracy: 0.6709\n",
            "Epoch 59/100\n",
            "312/312 [==============================] - 30s 95ms/step - loss: 1.2133 - accuracy: 0.6707\n",
            "Epoch 60/100\n",
            "312/312 [==============================] - 29s 94ms/step - loss: 1.1592 - accuracy: 0.6876\n",
            "Epoch 61/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 1.1376 - accuracy: 0.6892\n",
            "Epoch 62/100\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 1.1233 - accuracy: 0.6968\n",
            "Epoch 63/100\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 1.0904 - accuracy: 0.7052\n",
            "Epoch 64/100\n",
            "312/312 [==============================] - 30s 96ms/step - loss: 1.0619 - accuracy: 0.7125\n",
            "Epoch 65/100\n",
            "312/312 [==============================] - 30s 96ms/step - loss: 1.0446 - accuracy: 0.7177\n",
            "Epoch 66/100\n",
            "312/312 [==============================] - 30s 96ms/step - loss: 1.0235 - accuracy: 0.7181\n",
            "Epoch 67/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 0.9950 - accuracy: 0.7297\n",
            "Epoch 68/100\n",
            "312/312 [==============================] - 30s 96ms/step - loss: 0.9672 - accuracy: 0.7322\n",
            "Epoch 69/100\n",
            "312/312 [==============================] - 30s 96ms/step - loss: 0.9568 - accuracy: 0.7336\n",
            "Epoch 70/100\n",
            "312/312 [==============================] - 30s 96ms/step - loss: 0.9296 - accuracy: 0.7424\n",
            "Epoch 71/100\n",
            "312/312 [==============================] - 30s 96ms/step - loss: 0.9253 - accuracy: 0.7405\n",
            "Epoch 72/100\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 0.9136 - accuracy: 0.7489\n",
            "Epoch 73/100\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 0.8774 - accuracy: 0.7558\n",
            "Epoch 74/100\n",
            "312/312 [==============================] - 31s 99ms/step - loss: 0.8635 - accuracy: 0.7597\n",
            "Epoch 75/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 0.8332 - accuracy: 0.7715\n",
            "Epoch 76/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 0.8152 - accuracy: 0.7768\n",
            "Epoch 77/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 0.8105 - accuracy: 0.7710\n",
            "Epoch 78/100\n",
            "312/312 [==============================] - 30s 98ms/step - loss: 0.7916 - accuracy: 0.7774\n",
            "Epoch 79/100\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 0.7722 - accuracy: 0.7826\n",
            "Epoch 80/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 0.7681 - accuracy: 0.7773\n",
            "Epoch 81/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 0.7510 - accuracy: 0.7840\n",
            "Epoch 82/100\n",
            "312/312 [==============================] - 32s 102ms/step - loss: 0.7372 - accuracy: 0.7912\n",
            "Epoch 83/100\n",
            "312/312 [==============================] - 32s 102ms/step - loss: 0.7018 - accuracy: 0.8043\n",
            "Epoch 84/100\n",
            "312/312 [==============================] - 32s 101ms/step - loss: 0.6980 - accuracy: 0.8031\n",
            "Epoch 85/100\n",
            "312/312 [==============================] - 31s 99ms/step - loss: 0.6902 - accuracy: 0.8008\n",
            "Epoch 86/100\n",
            "312/312 [==============================] - 31s 100ms/step - loss: 0.6686 - accuracy: 0.8113\n",
            "Epoch 87/100\n",
            "312/312 [==============================] - 31s 100ms/step - loss: 0.6551 - accuracy: 0.8128\n",
            "Epoch 88/100\n",
            "312/312 [==============================] - 31s 100ms/step - loss: 0.6545 - accuracy: 0.8133\n",
            "Epoch 89/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 0.6238 - accuracy: 0.8215\n",
            "Epoch 90/100\n",
            "312/312 [==============================] - 30s 97ms/step - loss: 0.6293 - accuracy: 0.8201\n",
            "Epoch 91/100\n",
            "312/312 [==============================] - 30s 98ms/step - loss: 0.6280 - accuracy: 0.8199\n",
            "Epoch 92/100\n",
            "312/312 [==============================] - 31s 100ms/step - loss: 0.5863 - accuracy: 0.8291\n",
            "Epoch 93/100\n",
            "312/312 [==============================] - 30s 98ms/step - loss: 0.5689 - accuracy: 0.8389\n",
            "Epoch 94/100\n",
            "312/312 [==============================] - 31s 100ms/step - loss: 0.5614 - accuracy: 0.8384\n",
            "Epoch 95/100\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 0.5503 - accuracy: 0.8447\n",
            "Epoch 96/100\n",
            "312/312 [==============================] - 31s 99ms/step - loss: 0.5466 - accuracy: 0.8417\n",
            "Epoch 97/100\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 0.5325 - accuracy: 0.8427\n",
            "Epoch 98/100\n",
            "312/312 [==============================] - 31s 99ms/step - loss: 0.5110 - accuracy: 0.8499\n",
            "Epoch 99/100\n",
            "312/312 [==============================] - 31s 98ms/step - loss: 0.5180 - accuracy: 0.8491\n",
            "Epoch 100/100\n",
            "312/312 [==============================] - 30s 95ms/step - loss: 0.5004 - accuracy: 0.8516\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa8308ffd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RzJ3ab09dN2"
      },
      "source": [
        "# save model if needed for further training\n",
        "# from pickle import dump\n",
        "# model.save('model.h5')\n",
        "\n",
        "# save the tokenizer\n",
        "# dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMKnp49jFhSQ"
      },
      "source": [
        "Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhwsNVS7Fugl"
      },
      "source": [
        "# load the model\n",
        "# from tensorflow.keras.models import load_model\n",
        "# model = load_model('model.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "# tokenizer = load(open('tokenizer.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiO1q4dUb8Jg"
      },
      "source": [
        "seq_length = len(X[0])\n",
        "num2word = {v:k for k,v in word2num.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucDfgoVh_KxK"
      },
      "source": [
        "Randomly pick one sentence from the lyrics, and predict the next word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zie5no-eFx6E",
        "outputId": "a8795aca-8c8b-4b7f-a164-ff63b71e420b"
      },
      "source": [
        "from random import randint\n",
        "seed_text = X[randint(0,seq_length)]\n",
        "seed_text = ' '.join([num2word[w] for w in seed_text])\n",
        "print(' '+seed_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " streets i used to own \n",
            " \n",
            " i used to roll the dice \n",
            " feel the fear in my enemy's eyes \n",
            " listen as the crowd would sing \n",
            " now the old king is dead long live the king \n",
            " \n",
            " one minute i held the key \n",
            " next the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "mDjTE70ydR42",
        "outputId": "2c457710-6889-4f82-b695-48545c771a3e"
      },
      "source": [
        "encoded = [word2num[w] for w in re.findall(r'\\S+|\\n',seed_text)]\n",
        "encoded = np.array(encoded).reshape(1,-1)\n",
        "\n",
        "# predict probabilities for each word\n",
        "yhat = np.argmax(model.predict(encoded, verbose=0),axis = 1)\n",
        "num2word[yhat[0]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'walls'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZGJZc2qHfK2"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# generate a sequence from a language model\n",
        "def generate_seq(model, seq_length, seed_text, n_words):\n",
        "\tresult = list()\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = [word2num[w] for w in re.findall(r'\\S+|\\n',in_text)]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = np.argmax(model.predict(encoded), axis=-1)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in word2num.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArElbq1XT-nO",
        "outputId": "86bbb1f9-28a8-4682-a0c4-caf9cb8f4936"
      },
      "source": [
        "# generate new text\n",
        "generated = generate_seq(model, seq_length, seed_text, 50)\n",
        "print(' '+generated)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " walls were closed on me \n",
            " and i discovered the streets of baltimore \n",
            " \n",
            " well her heart was filled with gladness \n",
            " when she saw those city lights \n",
            " she said is a waterfall ah \n",
            " and every tear \n",
            " every teardrop is a waterfall \n",
            " \n",
            " every tear\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgTIaRuM4ww5"
      },
      "source": [
        "#### Compute lyrics with ryhming setences like coldplay's lyrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4quuBgLTQWvO"
      },
      "source": [
        "I found a package can be used to find rhyming words which luckily saved a lot of time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guLIkYc2H9zT",
        "outputId": "1ade5e6f-f6ac-458c-8ae9-4b76ec043e5e"
      },
      "source": [
        "# !pip install phyme"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting phyme\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/59/fd6ec3b00a31f721056d2411fa420400c3edb90487e3e9a2d3e2c3603566/Phyme-0.0.9.tar.gz (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 5.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: phyme\n",
            "  Building wheel for phyme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for phyme: filename=Phyme-0.0.9-cp36-none-any.whl size=1379057 sha256=d967f0015e38b82a1cb00d02d5aa7c6f2377b75fd12aa71146b66e8a5393fb2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/79/ca/e58a1f9509af3537f34c9c98ab6dfcf56f1b7cd40788c9a46d\n",
            "Successfully built phyme\n",
            "Installing collected packages: phyme\n",
            "Successfully installed phyme-0.0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAYrpBSmR5_f"
      },
      "source": [
        "from Phyme import Phyme\n",
        "ph = Phyme()\n",
        "# Try to get rhyming words of 'right'\n",
        "ph.get_partner_rhymes('right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrDuYA8vUvQE"
      },
      "source": [
        "from random import randint\n",
        "seed_text = X[randint(0,seq_length)]\n",
        "seed_text = ' '.join([num2word[w] for w in seed_text])\n",
        "coldplay_corpus = generate_seq(model, seq_length, seed_text, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUM-R9GtAUoR"
      },
      "source": [
        "seed_text = X[randint(0,seq_length)]\n",
        "seed_text = ' '.join([num2word[w] for w in seed_text])\n",
        "coldplay_corpus2 = generate_seq(model, seq_length, seed_text, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxTJdk15avFA"
      },
      "source": [
        "corpus = coldplay_corpus + coldplay_corpus2\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPxKIQeV5y2D"
      },
      "source": [
        "# Array of final word of each line of corpus\n",
        "final_corpus = coldplay_corpus.translate(str.maketrans(\"\", \"\", punctuation))\n",
        "lines = [l.strip() for l in final_corpus.split('\\n') if len(l)>1]\n",
        "list_of_words = [line.strip().split(\" \") for line in lines]\n",
        "# Try to clean some words from lyrics\n",
        "final_words = [w[-1] for w in list_of_words if len(w)>1]\n",
        "final_words = ['o' if w=='oooooo' else w for w in final_words]\n",
        "final_words = ['oh' if w=='oh…' else w for w in final_words]\n",
        "final_words = ['boom' if w=='baboomboom' else w for w in final_words]\n",
        "final_words = ['n' if w=='oooooooonnnn' else w for w in final_words]\n",
        "word_dict = {i:w for i, w in enumerate(final_words)}\n",
        "rhyme_dict = {}\n",
        "for i, word1 in word_dict.items():\n",
        "    try:\n",
        "      rhyming = ph.get_consonant_rhymes(word1).values()\n",
        "      rhyming_words = [j for sub in rhyming for j in sub] \n",
        "      rhymes = []\n",
        "    \n",
        "      for j, word2 in word_dict.items():\n",
        "          if word2!=word1 and word2 in rhyming_words:\n",
        "              print(j,word2)\n",
        "              rhymes.append(j)\n",
        "      rhyme_dict[i] = rhymes\n",
        "    except:\n",
        "      continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr2xyWhqCkm1"
      },
      "source": [
        "Here is the interesting part:\n",
        "Create a function to generate lyrics with structure of two couplets followed by two couplets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w6UCP8zpzfj"
      },
      "source": [
        "import random\n",
        "def generate_couplet(rhymes):\n",
        "    while 1:\n",
        "        i = random.randrange(len(rhymes.keys()))\n",
        "        if len(rhymes[i]) >= 1:\n",
        "            pool = rhymes[i] + [i]\n",
        "            print(pool)\n",
        "            print(word_dict[i] for i in pool)\n",
        "            samples = random.sample(pool, 2)\n",
        "            while lines[samples[0]] == lines[samples[1]]:\n",
        "                samples = random.samples(pool, 2)\n",
        "            return samples\n",
        "            print(samples)\n",
        "            print(word_dict[i] for i in samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzmNcZ2quxUw"
      },
      "source": [
        "def generate_lyrics(rhymes):\n",
        "    a = generate_couplet(rhymes)\n",
        "    b = generate_couplet(rhymes)\n",
        "    c = [random.randrange(len(rhymes.keys()))]\n",
        "    return a+b+c+c\n",
        "\n",
        "# Convert lyrics from index array to string\n",
        "def conv_lyrics(indices, lines):\n",
        "    lyric = \"\"\n",
        "    for i in indices:\n",
        "        lyric += lines[i] + \"\\n\"\n",
        "    return lyric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoY-kGYb-u3d"
      },
      "source": [
        "num_lyrics = 2\n",
        "lyrics_ryhme = []\n",
        "for _ in range(num_lyrics):\n",
        "  lyric_coldplay = conv_lyrics(generate_lyrics(rhyme_dict), lines)\n",
        "  lyrics_ryhme.append(lyric_coldplay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn9T59vtAANd",
        "outputId": "06f9e8ac-ce2f-4545-9b9d-7740410ef642"
      },
      "source": [
        "for i in lyrics_ryhme:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "be my mirror my sword and shield\n",
            "my missionaries in a foreign field\n",
            "i hear jerusalem bells are ringing\n",
            "roman cavalry choirs are singing\n",
            "its a wonderful life\n",
            "its a wonderful life\n",
            "\n",
            "i hear jerusalem bells are ringing\n",
            "roman cavalry choirs are singing\n",
            "my missionaries in a foreign field\n",
            "be my mirror my sword and shield\n",
            "never felt so alive\n",
            "never felt so alive\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFirIkhqQvgG"
      },
      "source": [
        "Final results are better than character-based model. We can explore more with models such as Attention."
      ]
    }
  ]
}